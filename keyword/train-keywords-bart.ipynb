{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad336725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# rich: for a better display on terminal\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"center\"),\n",
    "        Column(\"target_text\", justify=\"center\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "# training logger to log training progress\n",
    "training_logger = Table(\n",
    "    Column(\"Epoch\", justify=\"center\"),\n",
    "    Column(\"Steps\", justify=\"center\"),\n",
    "    Column(\"Loss\", justify=\"center\"),\n",
    "    title=\"Training Status\",\n",
    "    pad_edge=False,\n",
    "    box=box.ASCII,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bd013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  4 01:11:38 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    52W / 400W |      3MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   311W / 400W |  39212MiB / 40536MiB |     99%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   43C    P0   243W / 400W |  22926MiB / 40536MiB |     92%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    59W / 400W |   1682MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A    891770      C   /usr/bin/python3                39209MiB |\n",
      "|    2   N/A  N/A    882943      C   python3                         22923MiB |\n",
      "|    3   N/A  N/A   2480422      C   ...vs/preconds_v2/bin/python     1679MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "WARNING: infoROM is corrupted at gpu 0000:01:00.0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056720e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda:0' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23460e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.summ_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11276d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _ % 500 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c0f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    inputs = []\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "            \n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=512, \n",
    "                min_length = 250,\n",
    "              num_beams = 5,\n",
    "              no_repeat_ngram_size = 5,\n",
    "              #topp = 0.9,\n",
    "              #do_sample=True,\n",
    "              repetition_penalty=5.8, \n",
    "              length_penalty=1, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            input_text = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in ids]\n",
    "            if _%50==0:\n",
    "                outputs = model(\n",
    "                        input_ids=ids,\n",
    "                        attention_mask=mask,\n",
    "                        decoder_input_ids=y_ids,\n",
    "                        labels=lm_labels,\n",
    "                        )\n",
    "                loss = outputs[0]\n",
    "                console.print(f'Completed {_}')\n",
    "                console.print('loss: '+ str(loss))\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            inputs.extend(input_text)\n",
    "    return inputs, predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e75d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    inputs = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "            \n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=5120, \n",
    "                min_length = 250,\n",
    "              num_beams = 4,\n",
    "              no_repeat_ngram_size = 5,\n",
    "              #topp = 0.9,\n",
    "              #do_sample=True,\n",
    "              repetition_penalty=5.8, \n",
    "              length_penalty=1, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            input_text = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in ids]\n",
    "            if _%50==0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            inputs.extend(input_text)\n",
    "    return inputs, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491c6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T5Trainer(\n",
    "    dataframe, source_text, target_text, model_params, model, tokenizer, output_dir=\"./outputs/\"\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    \n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So x% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.998\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = YourDataSetClass(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "        console.log(f\"[Initiating Validation]...\\n\")\n",
    "        inputs, predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Input': inputs, \"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir, \"predictions\"+str(epoch)+\".csv\"))\n",
    "\n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    # evaluating test dataset\n",
    "    \n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
    "    )\n",
    "    console.print(\n",
    "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f22fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('all_stories_14_lines_new_mix_.json', errors='ignore').readlines()\n",
    "all_scary =  json.loads(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e98fece8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19267"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_scary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39eb5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_poetry_foundation = all_scary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c2c8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19267"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_poetry_foundation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb2ee1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = '.'\n",
    "mask_token = '<MASK>'\n",
    "eos_token = '</s>'\n",
    "\n",
    "X_titles = []\n",
    "y_keywords = []\n",
    "template = [mask_token, mask_token, mask_token]\n",
    "prompt = 'Generate keywords for the title: '\n",
    "title_set = []\n",
    "\n",
    "for poem in all_poetry_foundation:\n",
    "    title_set.append(poem['Theme'])\n",
    "    title = prompt + poem['Theme']\n",
    "    paddings = []\n",
    "    temp = []\n",
    "    count = 0\n",
    "    for key in poem['keywords']:\n",
    "        if key == ['<paragraph>']:\n",
    "            continue\n",
    "        count += 1\n",
    "        mask = template[:len(key)]\n",
    "        paddings.append('Keywords '+ str(count) + ': '+ str(mask) )        \n",
    "        temp.append('Keywords '+ str(count) + ': '+ str(key))\n",
    "\n",
    "    paddings = (\" \"+special_token+\" \").join(paddings).replace('<paragraph> ','')\n",
    "    temp = (\" \"+special_token+\" \").join(temp).replace('<paragraph> ','')\n",
    "\n",
    "    X_titles.append(title + '. ' + paddings+\" \"+eos_token)\n",
    "    #X_titles.append(title)\n",
    "    y_keywords.append(temp+\" \"+eos_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56985a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generate keywords for the title: The end of re...</td>\n",
       "      <td>Keywords 1: ['worked', 'reddit', 'long'] . Key...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generate keywords for the title: No one mourns...</td>\n",
       "      <td>Keywords 1: ['October', 'silence', 'rode'] . K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generate keywords for the title: `` Captain 's...</td>\n",
       "      <td>Keywords 1: ['Captain', 'Log', 'Recorded'] . K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generate keywords for the title: You develop r...</td>\n",
       "      <td>Keywords 1: ['magic', 'quantum-mechanical', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generate keywords for the title: You 've just ...</td>\n",
       "      <td>Keywords 1: ['hall', 'walk', 'quickly'] . Keyw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Generate keywords for the title: The end of re...   \n",
       "1  Generate keywords for the title: No one mourns...   \n",
       "2  Generate keywords for the title: `` Captain 's...   \n",
       "3  Generate keywords for the title: You develop r...   \n",
       "4  Generate keywords for the title: You 've just ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  Keywords 1: ['worked', 'reddit', 'long'] . Key...  \n",
       "1  Keywords 1: ['October', 'silence', 'rode'] . K...  \n",
       "2  Keywords 1: ['Captain', 'Log', 'Recorded'] . K...  \n",
       "3  Keywords 1: ['magic', 'quantum-mechanical', 'm...  \n",
       "4  Keywords 1: ['hall', 'walk', 'quickly'] . Keyw...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [X_titles, y_keywords]\n",
    "df = pd.DataFrame(np.array(data).T, columns = ['title', 'keywords'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d77d24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Generate keywords for the title: Wiggle your big toe.... Keywords 1: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 2: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 3: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 4: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 5: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 6: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 7: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 8: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 9: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 10: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 11: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 12: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 13: ['<MASK>', '<MASK>', '<MASK>'] . Keywords 14: ['<MASK>', '<MASK>', '<MASK>'] </s>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][10005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba1deec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Keywords 1: ['paralysis', 'suffer', 'pretty'] . Keywords 2: ['sleep', 'anxiety', 'high'] . Keywords 3: ['quick', 'times', 'crushing'] . Keywords 4: ['awake', 'hallucinating', 'calm'] . Keywords 5: ['Panic', 'made', 'harder'] . Keywords 6: ['Tarantino', 'back', 'picked'] . Keywords 7: ['sweat', 'glued', 'bed'] . Keywords 8: ['Starting', 'move', 'toe'] . Keywords 9: ['tonight', 'woke', 'freezing'] . Keywords 10: ['covers', 'air', 'sleep'] . Keywords 11: ['mattress', 'feel', 'usual'] . Keywords 12: ['instantly', 'anxiety', 'rushed'] . Keywords 13: ['nose', 'shut', 'pulled'] . Keywords 14: ['Wiggle', 'toe', 'big'] </s>\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords'][10005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f785cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's define model parameters specific to bart\n",
    "model_params = {\n",
    "    \"TASK\" : \"0503-mix\",\n",
    "    \"MODEL\": \"facebook/bart-large\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TRAIN_EPOCHS\": 6,  # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 3e-6,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 512,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebac280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenzier for encoding the text\n",
    "tokenizer = BartTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c92d2480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233d1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = BartForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99c19659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(df['title'][15005])\n",
    "\n",
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f23c1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Keywords 1: ['wrong', 'things', 'presence'] . Keywords 2: ['Silhouettes', 'yard', 'steps'] . Keywords 3: ['safe', 'feel', 'cozy'] . Keywords 4: ['air', 'lifted', 'today'] . Keywords 5: ['grounds', 'spirits', 'inhabit'] . Keywords 6: ['mist', 'wrong', 'deeply'] . Keywords 7: ['mind', 'cottage', 'engulfed'] . Keywords 8: ['legs', 'dog', 'tail'] . Keywords 9: ['reached', 'house', 'wall'] . Keywords 10: ['silence', 'air', 'shook'] . Keywords 11: ['rumble', 'thunder', 'localized'] . Keywords 12: ['animal', 'slumber', 'horrible'] . Keywords 13: ['cottage', 'glance', 'dashed'] . Keywords 14: ['ran', 'life', 'cabin'] </s>\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords'][15005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cb1463d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(df['keywords'][15005])\n",
    "\n",
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0cb4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= model_params['MODEL']+\"_batch_\"+ str(model_params['TRAIN_BATCH_SIZE']) + \"_lr_\"+ str(model_params['LEARNING_RATE'])+ model_params['TASK']\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c08696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01:16:27] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading facebook/bart-large<span style=\"color: #808000; text-decoration-color: #808000\">...</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-9-fe037b46d362&gt;:16</span>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01:16:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading facebook/bart-large\u001b[33m...\u001b[0m          \u001b[2m<ipython-input-9-fe037b46d362>\u001b[0m\u001b[2m:16\u001b[0m\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-9-fe037b46d362&gt;:21</span>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                          \u001b[2m<ipython-input-9-fe037b46d362>\u001b[0m\u001b[2m:21\u001b[0m\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                         Sample Data                                         </span>\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                source_text                  </span>|<span style=\"font-weight: bold\">                 target_text                 </span>|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "|Generate keywords for the title: The end of  |  Keywords 1: ['worked', 'reddit', 'long'] . |\n",
       "| reddit .. Keywords 1: ['&lt;MASK&gt;', '&lt;MASK&gt;',  |   Keywords 2: ['AMA', 'crash', 'worked'] .  |\n",
       "|'&lt;MASK&gt;'] . Keywords 2: ['&lt;MASK&gt;', '&lt;MASK&gt;', | Keywords 3: ['AMA', 'typed', 'furiously'] . |\n",
       "|'&lt;MASK&gt;'] . Keywords 3: ['&lt;MASK&gt;', '&lt;MASK&gt;', |      Keywords 4: ['screen', 'honestly',     |\n",
       "|'&lt;MASK&gt;'] . Keywords 4: ['&lt;MASK&gt;', '&lt;MASK&gt;', | 'thought'] . Keywords 5: ['myspace', 'year',|\n",
       "|'&lt;MASK&gt;'] . Keywords 5: ['&lt;MASK&gt;', '&lt;MASK&gt;', |   'ago'] . Keywords 6: ['days', 'myspace',  |\n",
       "|'&lt;MASK&gt;'] . Keywords 6: ['&lt;MASK&gt;', '&lt;MASK&gt;', |  'sight'] . Keywords 7: ['paper', 'stupid', |\n",
       "|'&lt;MASK&gt;'] . Keywords 7: ['&lt;MASK&gt;', '&lt;MASK&gt;', |       'past'] . Keywords 8: ['People',      |\n",
       "|'&lt;MASK&gt;'] . Keywords 8: ['&lt;MASK&gt;', '&lt;MASK&gt;', |    'rediscover', 'awesome'] . Keywords 9:   |\n",
       "|'&lt;MASK&gt;'] . Keywords 9: ['&lt;MASK&gt;', '&lt;MASK&gt;', |  ['People', 'flock', 'back'] . Keywords 10: |\n",
       "|    '&lt;MASK&gt;'] . Keywords 10: ['&lt;MASK&gt;',      |  ['reddit', 'today', 'day'] . Keywords 11:  |\n",
       "|     '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 11:      | ['Today', 'shut', 'servers'] . Keywords 12: |\n",
       "| ['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords   |  ['Tomorrow', 'reddit', 'thing'] . Keywords |\n",
       "|    12: ['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] .     |    13: ['hours', 'reddit', 'employee'] .    |\n",
       "|Keywords 13: ['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;']  |     Keywords 14: ['remember', 'memory',     |\n",
       "|    . Keywords 14: ['&lt;MASK&gt;', '&lt;MASK&gt;',      |               'favorite'] &lt;/s&gt;              |\n",
       "|               '&lt;MASK&gt;'] &lt;/s&gt;                |                                             |\n",
       "|  Generate keywords for the title: No one    | Keywords 1: ['October', 'silence', 'rode'] .|\n",
       "| mourns the deaths of monsters. Keywords 1:  | Keywords 2: ['cramped', 'stuffy', 'crew'] . |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 2: |  Keywords 3: ['Slim', 'captain', 'cigar'] . |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 3: | Keywords 4: ['John', 'hull', 'listening'] . |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 4: | Keywords 5: ['operation', 'memory', 'year'] |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 5: |     . Keywords 6: ['bravery', 'courage',    |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 6: |    'strength'] . Keywords 7: ['America',    |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 7: |     'time', 'rebuilding'] . Keywords 8:     |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 8: | ['Thirty', 'headset', 'call'] . Keywords 9: |\n",
       "|['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 9: |   ['John', 'life', 'suit'] . Keywords 10:   |\n",
       "| ['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords   | ['watched', 'rituals', 'looked'] . Keywords |\n",
       "|    10: ['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] .     |   11: ['dull', 'emotionless', 'visors'] .   |\n",
       "|Keywords 11: ['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;']  |  Keywords 12: ['rose', 'cradling', 'arm'] . |\n",
       "|    . Keywords 12: ['&lt;MASK&gt;', '&lt;MASK&gt;',      |     Keywords 13: ['Alright', 'Seattle',     |\n",
       "|    '&lt;MASK&gt;'] . Keywords 13: ['&lt;MASK&gt;',      |     'listen'] . Keywords 14: ['Reports',    |\n",
       "|     '&lt;MASK&gt;', '&lt;MASK&gt;'] . Keywords 14:      |        'underground', 'tunnels'] &lt;/s&gt;       |\n",
       "|    ['&lt;MASK&gt;', '&lt;MASK&gt;', '&lt;MASK&gt;'] &lt;/s&gt;      |                                             |\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                         Sample Data                                         \u001b[0m\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                source_text                 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                target_text                 \u001b[0m|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "|Generate keywords for the title: The end of  |  Keywords 1: ['worked', 'reddit', 'long'] . |\n",
       "| reddit .. Keywords 1: ['<MASK>', '<MASK>',  |   Keywords 2: ['AMA', 'crash', 'worked'] .  |\n",
       "|'<MASK>'] . Keywords 2: ['<MASK>', '<MASK>', | Keywords 3: ['AMA', 'typed', 'furiously'] . |\n",
       "|'<MASK>'] . Keywords 3: ['<MASK>', '<MASK>', |      Keywords 4: ['screen', 'honestly',     |\n",
       "|'<MASK>'] . Keywords 4: ['<MASK>', '<MASK>', | 'thought'] . Keywords 5: ['myspace', 'year',|\n",
       "|'<MASK>'] . Keywords 5: ['<MASK>', '<MASK>', |   'ago'] . Keywords 6: ['days', 'myspace',  |\n",
       "|'<MASK>'] . Keywords 6: ['<MASK>', '<MASK>', |  'sight'] . Keywords 7: ['paper', 'stupid', |\n",
       "|'<MASK>'] . Keywords 7: ['<MASK>', '<MASK>', |       'past'] . Keywords 8: ['People',      |\n",
       "|'<MASK>'] . Keywords 8: ['<MASK>', '<MASK>', |    'rediscover', 'awesome'] . Keywords 9:   |\n",
       "|'<MASK>'] . Keywords 9: ['<MASK>', '<MASK>', |  ['People', 'flock', 'back'] . Keywords 10: |\n",
       "|    '<MASK>'] . Keywords 10: ['<MASK>',      |  ['reddit', 'today', 'day'] . Keywords 11:  |\n",
       "|     '<MASK>', '<MASK>'] . Keywords 11:      | ['Today', 'shut', 'servers'] . Keywords 12: |\n",
       "| ['<MASK>', '<MASK>', '<MASK>'] . Keywords   |  ['Tomorrow', 'reddit', 'thing'] . Keywords |\n",
       "|    12: ['<MASK>', '<MASK>', '<MASK>'] .     |    13: ['hours', 'reddit', 'employee'] .    |\n",
       "|Keywords 13: ['<MASK>', '<MASK>', '<MASK>']  |     Keywords 14: ['remember', 'memory',     |\n",
       "|    . Keywords 14: ['<MASK>', '<MASK>',      |               'favorite'] </s>              |\n",
       "|               '<MASK>'] </s>                |                                             |\n",
       "|  Generate keywords for the title: No one    | Keywords 1: ['October', 'silence', 'rode'] .|\n",
       "| mourns the deaths of monsters. Keywords 1:  | Keywords 2: ['cramped', 'stuffy', 'crew'] . |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 2: |  Keywords 3: ['Slim', 'captain', 'cigar'] . |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 3: | Keywords 4: ['John', 'hull', 'listening'] . |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 4: | Keywords 5: ['operation', 'memory', 'year'] |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 5: |     . Keywords 6: ['bravery', 'courage',    |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 6: |    'strength'] . Keywords 7: ['America',    |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 7: |     'time', 'rebuilding'] . Keywords 8:     |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 8: | ['Thirty', 'headset', 'call'] . Keywords 9: |\n",
       "|['<MASK>', '<MASK>', '<MASK>'] . Keywords 9: |   ['John', 'life', 'suit'] . Keywords 10:   |\n",
       "| ['<MASK>', '<MASK>', '<MASK>'] . Keywords   | ['watched', 'rituals', 'looked'] . Keywords |\n",
       "|    10: ['<MASK>', '<MASK>', '<MASK>'] .     |   11: ['dull', 'emotionless', 'visors'] .   |\n",
       "|Keywords 11: ['<MASK>', '<MASK>', '<MASK>']  |  Keywords 12: ['rose', 'cradling', 'arm'] . |\n",
       "|    . Keywords 12: ['<MASK>', '<MASK>',      |     Keywords 13: ['Alright', 'Seattle',     |\n",
       "|    '<MASK>'] . Keywords 13: ['<MASK>',      |     'listen'] . Keywords 14: ['Reports',    |\n",
       "|     '<MASK>', '<MASK>'] . Keywords 14:      |        'underground', 'tunnels'] </s>       |\n",
       "|    ['<MASK>', '<MASK>', '<MASK>'] </s>      |                                             |\n",
       "+-------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19267</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m19267\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19228</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m19228\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m39\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-9-fe037b46d362&gt;:79</span>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                      \u001b[2m<ipython-input-9-fe037b46d362>\u001b[0m\u001b[2m:79\u001b[0m\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01:29:19] </span><span style=\"font-weight: bold\">[</span>Initiating Validation<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-9-fe037b46d362&gt;:83</span>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01:29:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Validation\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                       \u001b[2m<ipython-input-9-fe037b46d362>\u001b[0m\u001b[2m:83\u001b[0m\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">loss: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6130</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "loss: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1.6130\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01:42:57] </span><span style=\"font-weight: bold\">[</span>Initiating Validation<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-9-fe037b46d362&gt;:83</span>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01:42:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Validation\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                       \u001b[2m<ipython-input-9-fe037b46d362>\u001b[0m\u001b[2m:83\u001b[0m\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">loss: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5891</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "loss: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1.5891\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   |  500  | tensor(1.5865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   |  500  | tensor(1.5865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   |  500  | tensor(1.5865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   | 1000  | tensor(1.6089, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   |  500  | tensor(1.5865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   | 1000  | tensor(1.6089, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   |  500  | tensor(1.5865, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   | 1000  | tensor(1.6089, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "|  2   | 1500  | tensor(1.5820, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.3423, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   |  500  | tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1000  | tensor(1.7483, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 1500  | tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  0   | 2000  | tensor(1.5783, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |   0   | tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   |  500  | tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1000  | tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 1500  | tensor(1.6397, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  1   | 2000  | tensor(1.6508, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   |   0   | tensor(1.5677, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   |  500  | tensor(1.5865, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   | 1000  | tensor(1.6089, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "|  2   | 1500  | tensor(1.5820, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#GPU usage: 37186 MB for T5 large, batch_size 3\n",
    "#GPU usage:  MB for T5 base, batch_size 12\n",
    "T5Trainer(\n",
    "    dataframe=df,\n",
    "    source_text=\"title\",\n",
    "    target_text=\"keywords\",\n",
    "    model_params=model_params,\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    output_dir = output_dir\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
