{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c100ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPTNeoForCausalLM, GPTNeoModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pronouncing\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Download Finetuned GPT-Neo\n",
    "# Set the random seed to a fixed value to get reproducible results \n",
    "torch.manual_seed(42)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", \n",
    "                                          bos_token=\"<|startoftext|>\",\n",
    "                            eos_token=\"<|endoftext|>\",\n",
    "                            pad_token=\"<|pad|>\")\n",
    "\n",
    "# Download the pre-trained GPT-Neo model and transfer it to the GPU\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"FigoMe/news-gpt-neo-1.3B-keywords-line-by-line-reverse\").cuda()\n",
    "# Resize the token embeddings because we've just added 3 new tokens \n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def get_stress(phone):\n",
    "    stress = []\n",
    "    for s in phone.split():\n",
    "        if s[-1].isdigit():\n",
    "            if s[-1] == '2':\n",
    "                stress.append(0)\n",
    "            else:\n",
    "                stress.append(int(s[-1]))\n",
    "    return stress\n",
    "\n",
    "def alternating(stress):\n",
    "    #Check if the stress and unstress are alternating\n",
    "    check1 = len(set(stress[::2])) <= 1 and (len(set(stress[1::2])) <= 1)\n",
    "    check2 = len(set(stress)) == 2 if len(stress) >=2 else True\n",
    "    return (check1 and check2)\n",
    "\n",
    "def get_phones(rhyme_word):\n",
    "    phone = pronouncing.phones_for_word(rhyme_word)[0]\n",
    "    stress = get_stress(phone)\n",
    "    p_state = stress[0]\n",
    "    n_syllables = len(stress)\n",
    "    return p_state, n_syllables\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits: Tensor,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    filter_value: float = -float(\"Inf\"),\n",
    "    min_tokens_to_keep: int = 1,\n",
    "    return_index = False\n",
    ") -> Tensor:\n",
    "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    Args:\n",
    "        logits: logits distribution shape (batch size, vocabulary size)\n",
    "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = logits >= torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = indices_keep[0].tolist()\n",
    "        indices_keep = [i for i,x in enumerate(indices_keep) if x == True]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    if return_index == True:\n",
    "        return logits, indices_keep\n",
    "    return logits\n",
    "\n",
    "\n",
    "def reverse_order(line):\n",
    "    line = line.replace(', ', ' , ')\n",
    "    words = line.split()\n",
    "    return ' '.join(reversed(words)).replace(' , ', ', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ac6397af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_list = ['that','is','of','the','it','a','as','with','like','go','to','on','in','at','are','and']\n",
    "def check_either_stress(stress, source_word, loose = True):\n",
    "    if loose and source_word in loose_list:\n",
    "        return True\n",
    "    if len(stress) == 1 and len(pronouncing.phones_for_word(source_word))>1:\n",
    "                    phone0 = pronouncing.phones_for_word(source_word)[0]\n",
    "                    phone1 = pronouncing.phones_for_word(source_word)[1]\n",
    "                    stress0 = [int(s[-1]) for s in phone0.split() if s[-1].isdigit()]\n",
    "                    stress1 = [int(s[-1]) for s in phone1.split() if s[-1].isdigit()]\n",
    "                    if stress0+stress1 ==1 and stress0*stress1 == 0:\n",
    "                        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e76c5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "gpt2_tokenizer  = AutoTokenizer.from_pretrained('gpt2-large')\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fcc9a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularBeamSearch(prompts):\n",
    "\t'''\n",
    "\tBeam search that considers the coherence by adding a new variable: previously_generated_lines\n",
    "\t'''\n",
    "\tBeamScorer = {}\n",
    "\tfor sentence in prompts:\n",
    "\t\tloss = score_gpt2(sentence)\n",
    "\t\tBeamScorer[sentence] = [loss]\n",
    "\tanswers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "\tnew_prompts = [ans[0] for ans in answers]\n",
    "\treturn new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8f6bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_gpt2(sentence, normalize = True):\n",
    "\t'''\n",
    "\tScore a single sentence using the vanilla gpt2 model finetuned on lyrics\n",
    "\tThe default setting is to normalize because we won't face the issue mentioned in function \"score\".\n",
    "\t'''\n",
    "\ttokens_tensor = gpt2_tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")[0].cuda()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss = gpt2_model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "\tif normalize:\n",
    "\t\treturn loss/len(tokens_tensor)\n",
    "\telse:\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "019a1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myBeamSearch(prompts, all_states, all_n_sys, all_keywords, beam_size = 5):\n",
    "    BeamScorer = {}\n",
    "    return_seq, return_stt, return_sys, return_key = [], [], [], []\n",
    "    for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "        loss = score(sentence)\n",
    "        BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "    answers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=True)\n",
    "    new_prompts = [ans[0] for ans in answers]\n",
    "    new_p_states = [ans[1][1] for ans in answers]\n",
    "    new_n_sys = [ans[1][2] for ans in answers]\n",
    "    new_keywords = [ans[1][3] for ans in answers]\n",
    "    l = len(new_prompts)\n",
    "    if l > beam_size:\n",
    "        return_seq += new_prompts[0:beam_size]\n",
    "        return_stt += new_p_states[0:beam_size]\n",
    "        return_sys += new_n_sys[0:beam_size]\n",
    "        return_key += new_keywords[0:beam_size]\n",
    "    else:\n",
    "        return_seq +=new_prompts\n",
    "        return_stt += new_p_states\n",
    "        return_sys += new_n_sys\n",
    "        return_key += new_keywords\n",
    "    return return_seq,return_stt, return_sys, return_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e301bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(input_ids1, temperature = 0.85, topk = 100, n_sample=10, device = 'cuda:0'):\n",
    "    current_word = 0\n",
    "    original = tokenizer.decode(input_ids1[0])\n",
    "    for _ in range(1):\n",
    "        outputs1 = model(input_ids1)\n",
    "        #print(outputs1)\n",
    "        next_token_logits1 = outputs1[0][:, -1, :]\n",
    "        next_token_logits1 = top_k_top_p_filtering(next_token_logits1, top_k=topk)\n",
    "        logit_zeros = torch.zeros(len(next_token_logits1)).cuda()\n",
    "        #logit_zeros = torch.zeros(len(next_token_logits1), device=device)\n",
    "\n",
    "        next_token_logits = next_token_logits1 * temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=n_sample).squeeze(1)\n",
    "        #unfinished_sents = torch.ones(1, dtype=torch.long, device=device)\n",
    "        unfinished_sents = torch.ones(1, dtype=torch.long).cuda()\n",
    "        tokens_to_add = next_tokens * unfinished_sents + tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "\n",
    "        temp = []\n",
    "        for i in range(len(input_ids1)):\n",
    "            temp +=[torch.cat([input_ids1[i].reshape(1,-1), token_to_add.reshape(1,-1)], dim=-1) for token_to_add in tokens_to_add[i]]\n",
    "        input_ids1 = torch.stack(temp).view(len(temp),-1)\n",
    "        # decode the generated token ids to natural words\n",
    "        results = []\n",
    "        input_ids1_l = []\n",
    "        for input_id1 in input_ids1:\n",
    "            gen = tokenizer.decode(input_id1).replace(original,'').strip(' ')\n",
    "            if len(gen.split()) >0:\n",
    "                gen = gen.split()[0]\n",
    "                gen = gen.lower()\n",
    "                if gen not in results:\n",
    "                    results.append(gen)\n",
    "        return results\n",
    "        '''\n",
    "        if tokenizer.decode(tokens_to_add[0])[0] == ' ':\n",
    "            if current_word ==1:\n",
    "                return tokenizer.decode(input_ids1[0]).split()[-1], False\n",
    "            current_word += 1\n",
    "        input_ids1 = torch.cat([input_ids1, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sentence, normalize = True):\n",
    "\t'''\n",
    "\tScore a single sentence using the plan-to-lyrics model.\n",
    "\tThe recommended setting is to NOT normalize, because the input sentence is very long: it contains the title, planed keywords, and previously generated lines. \n",
    "\tIn addition, the candidate sentences contain the same prefix (i.e., the title, planed keywords, and previously generated lines) and only differ in the currently generated line.\n",
    "\tNormaling means dividing the loss by a large factor which may result in similarity accross different candidate sentences.\n",
    "\t'''\n",
    "\ttokens_tensor = tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")[0].cuda()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss = model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "\tif normalize:\n",
    "\t\treturn loss/len(tokens_tensor)\n",
    "\telse:\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228febb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "090524aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_character_word = ['i','a']\n",
    "forbidden_words = ['dona','er','ira','ia',\"'s\",\"'m\",\"hmm\",\"mm\"]\n",
    "def get_valid_samples(prompt, p_state, n_syllables, keywords, n_sample=30, n_cands=5):\n",
    "    #if n_syllables == 10 or n_syllables==11:\n",
    "    if n_syllables == 10:\n",
    "        return [prompt], [p_state], [n_syllables], [keywords]\n",
    "    elif n_syllables > 10:\n",
    "        return [], [], [],[]\n",
    "    states = []\n",
    "    all_n_syl = []\n",
    "    \n",
    "    prompts = []\n",
    "    all_keywords= [] \n",
    "    #insert the keyword whenever possible\n",
    "    for source_word in keywords:\n",
    "        phone = pronouncing.phones_for_word(source_word)[0]\n",
    "        stress = get_stress(phone)\n",
    "        if not alternating(stress):\n",
    "            continue\n",
    "\n",
    "        #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "        flag = check_either_stress(stress, source_word)\n",
    "\n",
    "        if stress[-1] == 1- p_state or flag:\n",
    "            #print(source_word)\n",
    "            states.append(stress[0])\n",
    "            all_n_syl.append(n_syllables+len(stress))\n",
    "            prompts.append(prompt+ ' ' + source_word )\n",
    "            copy = keywords.copy()\n",
    "            copy.remove(source_word)\n",
    "            all_keywords.append(copy)    \n",
    "    \n",
    "    #The normal process of decoding\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    tokens = generate_next_word(input_ids, n_sample=n_sample)\n",
    "    #print(tokens)\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if (len(token) == 1 and token not in single_character_word) or token in forbidden_words:\n",
    "            continue\n",
    "        if token not in prompt:\n",
    "            try:\n",
    "                phone = pronouncing.phones_for_word(token)[0]\n",
    "                stress = get_stress(phone)\n",
    "            except:\n",
    "                continue\n",
    "            if (not alternating(stress)) or (len(stress)==0):\n",
    "                continue\n",
    "\n",
    "            #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "            flag = check_either_stress(stress, token)\n",
    "\n",
    "            if (stress[-1] == 1- p_state) or flag:\n",
    "                tokens.append(token)\n",
    "                if stress[-1] == 1- p_state:\n",
    "                    states.append(stress[0])\n",
    "                elif flag:\n",
    "                    states.append(1- p_state)\n",
    "                all_n_syl.append(n_syllables+len(stress))\n",
    "                prompts.append(prompt+ ' ' + token )\n",
    "                all_keywords.append(keywords)\n",
    "                if len(prompts)>= n_cands:\n",
    "                    return prompts, states, all_n_syl, all_keywords\n",
    "    return prompts, states, all_n_syl, all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e0b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_seasons_story_line = [\n",
    "['snow', 'falling', 'future'],\n",
    "['winter', 'is', 'coming'],\n",
    "['gather', 'honest', 'humor'],\n",
    "['spring', 'happy', 'blooming'],\n",
    "['air', 'heat', 'warm'],\n",
    "['little', 'birds', 'may'],\n",
    "['flowers', 'leaves', 'storm'],\n",
    "['summer','moon', 'day'],\n",
    "['blue', 'sky', 'clouds'],\n",
    "['sudden', 'rain', 'thunder'],\n",
    "['Summer', 'fill', 'crowds'],\n",
    "['Spring', 'no', 'wonder'],\n",
    "['seasons','years', 'keep'],\n",
    "['future', 'months', 'reap']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5406ae5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'falling', 'future']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [00:06<01:27,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['means that falling parents distant future', 'falling melting snow uncertain future', 'clearly falling even better future', 'is and falling something better future', 'snow the falling parents distant future', 'old and falling parents distant future', 'falling soon remote uncertain future', 'game itself is falling better future', 'air itself is falling better future', 'night itself is falling better future', 'sun itself is falling better future', 'school and falling parents distant future', 'cold and falling parents distant future', 'from that falling parents distant future', 'saw that falling parents distant future', 'see that falling parents distant future', 'news and falling parents distant future', 'always falling something better future', 'likely falling parents distant future', 'never falling parents distant future']\n",
      "['winter', 'is', 'coming']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [00:14<01:24,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ask around with yes and also coming', 'warning but with yes and also coming', 'find the platform yes and also coming', 'winter colder air and also coming', 'skating platform yes and also coming', 'winter winters is and also coming', 'winter platform yes and also coming', 'doom impending danger also coming', 'winter mortal danger also coming', 'your impending danger also coming', 'winter senses something always coming', 'winter sensing something always coming']\n",
      "['gather', 'honest', 'humor']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [00:18<01:06,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['others gather longer having humor', 'gather never longer having humor', 'honest never longer having humor', 'gather honest always having humor', 'gather seasons longer having humor', 'gather honest longer having humor', 'honest seasons longer having humor', 'honest gather longer having humor', 'many seasons longer having humor', 'being honest longer having humor', 'never honest longer having humor', 'children gather longer having humor', 'always gather honest having humor', 'often gather honest having humor', 'only gather honest having humor', 'better gather longer having humor', 'even gather honest rather humor', 'better gather honest rather humor']\n",
      "['spring', 'happy', 'blooming']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:24<00:59,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['several thousand flowers happy blooming', 'cheerful yellow garden always blooming', 'buzzing head like flowers happy blooming', 'spring like really happy flower blooming', 'spring with happy lovely garden blooming', 'truly really happy flower blooming', 'met with thousand flowers happy blooming', 'colors really rather happy blooming', 'town with happy lovely garden blooming', 'lovely garden rather happy blooming', 'planting season rather happy blooming', 'seven thousand flowers happy blooming', 'always really rather happy blooming', 'happy country lovely garden blooming', 'into country lovely garden blooming', 'spring already lovely garden blooming', 'into happy lovely garden blooming', 'given season very happy blooming', 'season really rather happy blooming', 'empty garden rather happy blooming']\n",
      "['air', 'heat', 'warm']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [00:29<00:51,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['especially after slowly getting warm', 'with children rated over keeping warm', 'with heat refreshing climate keeping warm', 'already bearing over getting warm', 'like heat eternal purpose keeping warm']\n",
      "['little', 'birds', 'may']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [00:34<00:41,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "['believe however little many may', 'however comfort little many may']\n",
      "['flowers', 'leaves', 'storm']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [00:39<00:37,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recycled metal pillar cater storm', 'considered normal after flowers storm', 'like flowers metal pillar cater storm', 'like lonely flowers pillar cater storm']\n",
      "['summer', 'moon', 'day']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "[]\n",
      "['summer', 'moon', 'day']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "[]\n",
      "['summer', 'moon', 'day']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [00:50<00:43,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "['extremely soothing summer mood like day', 'another humid summer summers day']\n",
      "['blue', 'sky', 'clouds']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [00:56<00:32,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "['creating new expecting yellow clouds', 'expected blue expecting yellow clouds']\n",
      "['sudden', 'rain', 'thunder']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [00:57<00:20,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lightning started rather sudden thunder', 'almost always pretty sudden thunder', 'maybe only really sudden thunder', 'seeing something really sudden thunder', 'notice something really sudden thunder', 'started only really sudden thunder', 'started almost very sudden thunder', 'rather something really sudden thunder', 'started rather pretty sudden thunder']\n",
      "['Summer', 'fill', 'crowds']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [01:03<00:15,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "['already closing seasons Summer crowds']\n",
      "['Spring', 'no', 'wonder']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [01:05<00:08,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nearly always very peaceful wonder', 'truly no beginnings only wonder', 'weather always very peaceful wonder', 'very oddly even peaceful wonder', 'always no beginnings only wonder', 'ended oddly even peaceful wonder', 'into Spring beginnings only wonder']\n",
      "['seasons', 'years', 'keep']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [01:10<00:04,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "['remaining years conditions therefore keep', 'although exact conditions therefore keep']\n",
      "['future', 'months', 'reap']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:14<00:00,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "['intentions threaten even children reap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "example_title = 'Four Seasons'\n",
    "beam_size=20\n",
    "previous = ''\n",
    "enforce_keywords = False\n",
    "for kws in tqdm(four_seasons_story_line):\n",
    "    success=False\n",
    "    n_sample = 30\n",
    "    while success != True:\n",
    "        print(kws)\n",
    "        rhyme_word = kws[-1]\n",
    "        prefix =  '''Keywords: ''' + '; '.join(kws) +'. Sentence in reverse order: '\n",
    "        prompt = '''<|startoftext|> Title: ''' + example_title + ' ' + previous + prefix + rhyme_word\n",
    "        p_state, n_syllables = get_phones(rhyme_word)\n",
    "        result_list = []\n",
    "        i=0\n",
    "        prompts, all_states, all_n_sys, all_keywords = get_valid_samples(prompt,p_state, n_syllables, keywords = kws[:2], n_sample=n_sample,n_cands=5)\n",
    "        while i<7:\n",
    "            print(i)\n",
    "            new_prompts, new_states, new_n_sys, new_keywords = [], [], [], []\n",
    "            for prompt, p_state, n_syllables, keyword in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "                t_p, t_state, t_sys, t_keywords = get_valid_samples(prompt, p_state, n_syllables, keyword,n_sample=n_sample)\n",
    "                new_prompts+=t_p\n",
    "                new_states+=t_state\n",
    "                new_n_sys+=t_sys\n",
    "                new_keywords+=t_keywords\n",
    "            prompts, all_states, all_n_sys, all_keywords = new_prompts, new_states, new_n_sys, new_keywords\n",
    "\n",
    "            prompts, all_states, all_n_sys, all_keywords = myBeamSearch(prompts,all_states, all_n_sys, all_keywords, beam_size=beam_size)\n",
    "            i += 1\n",
    "        correct_prompts = [reverse_order(p.split('order: ')[1]) for p in prompts]\n",
    "        result_list = regularBeamSearch(correct_prompts)\n",
    "        print(result_list)\n",
    "        if len(result_list)!=0:\n",
    "            success=True\n",
    "            found = False\n",
    "            if enforce_keywords:\n",
    "                for r in result_list:\n",
    "                    if kws[0] in r and kws[1] in r:\n",
    "                        previous = previous + r + ','\n",
    "                        found = True\n",
    "                        break\n",
    "            if found == False:\n",
    "                for r in result_list:\n",
    "                    if kws[0] in r or kws[1] in r:\n",
    "                        previous = previous + r + ','\n",
    "                        found = True\n",
    "                        break\n",
    "            if found == False:\n",
    "                previous = previous + result_list[0]+','\n",
    "                n_sample = n_sample*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "46d79f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Enforce keywords:\n",
      "warmer sun and we falling better future,\n",
      "clearly mean winter weather always coming,\n",
      "trying being honest see is humor,\n",
      "very happy was already blooming,\n",
      "humid air amazing power staying warm,\n",
      "sneaky little squirrel maybe even may,\n",
      "refreshing autumn flowers chilly storm,\n",
      "summer moon enjoying any special day,\n",
      "citrus orange sky remember seeing clouds,\n",
      "notice something really sudden thunder,\n",
      "peaceful Summer county over quiet crowds,\n",
      "tourist market farmers also wonder,\n",
      "seasons old exciting novel title keep,\n",
      "thrilling news awaited future only reap,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Not Enforce keywords:')\n",
    "\n",
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "172c9d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Enforce keywords:\n",
      "means that falling parents distant future,\n",
      "winter colder air and also coming,\n",
      "others gather longer having humor,\n",
      "several thousand flowers happy blooming,\n",
      "with heat refreshing climate keeping warm,\n",
      "believe however little many may,\n",
      "considered normal after flowers storm,\n",
      "extremely soothing summer mood like day,\n",
      "expected blue expecting yellow clouds,\n",
      "lightning started rather sudden thunder,\n",
      "already closing seasons Summer crowds,\n",
      "truly no beginnings only wonder,\n",
      "remaining years conditions therefore keep,\n",
      "intentions threaten even children reap,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Not Enforce keywords:')\n",
    "\n",
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1fe87601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce keywords:\n",
      "snow the falling new uncertain future,\n",
      "there already is and always coming,\n",
      "gather being honest any humor,\n",
      "spring amazing looking happy blooming,\n",
      "brutal winter heat surprises rather warm,\n",
      "singing many other merry little may,\n",
      "flowers leaves discovered how unwanted storm,\n",
      "refreshing sunny early summer day,\n",
      "colors sky expected fluffy yellow clouds,\n",
      "rain another sudden rolling thunder,\n",
      "follows Summer fill expecting festive crowds,\n",
      "really no whats truly peaceful wonder,\n",
      "busy your entire seasons parcel keep,\n",
      "future months potential harvest only reap,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Enforce keywords:')\n",
    "\n",
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d7cdfa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow and own falling dark endless future,\n",
      "follows is been something always coming,\n",
      "elders never gather honest humor,\n",
      "sunny spring already happy blooming,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a6f666cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow the never falling better future,\n",
      "winters truly is and really coming,\n",
      "gather honest very little humor,\n",
      "happy early spring already blooming,\n",
      "hunting season almost always rather warm,\n",
      "lonely fellow other pretty little may,\n",
      "purple flowers yellow leaves impending storm,\n",
      "summer harvest moon another special day,\n",
      "crimson thunder bearing over angry clouds,\n",
      "sudden heavy rain approaching thunder,\n",
      "colors Summer fill erupted into crowds,\n",
      "excitement yet beginnings only wonder,\n",
      "seasons recent years successful title keep,\n",
      "many months exciting looking future reap,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "930a14ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surely snow the falling better future,\n",
      "winters really is and always coming,\n",
      "helping people gather honest humor,\n",
      "happy pretty yellow flowers blooming,\n",
      "hunting season also slowly getting warm,\n",
      "started singing even fluffy little may,\n",
      "purple flowers autumn leaves impending storm,\n",
      "summer moon another very special day,\n",
      "sweater only barely hiding any clouds,\n",
      "sudden heavy rain approaching thunder,\n",
      "Summer fill already vibrant city crowds,\n",
      "lonely local corner never wonder,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91c04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
