{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c100ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPTNeoForCausalLM, GPTNeoModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pronouncing\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Download Finetuned GPT-Neo\n",
    "# Set the random seed to a fixed value to get reproducible results \n",
    "torch.manual_seed(42)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", \n",
    "                                          bos_token=\"<|startoftext|>\",\n",
    "                            eos_token=\"<|endoftext|>\",\n",
    "                            pad_token=\"<|pad|>\")\n",
    "\n",
    "# Download the pre-trained GPT-Neo model and transfer it to the GPU\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"FigoMe/news-gpt-neo-1.3B-keywords-line-by-line-reverse\").cuda()\n",
    "# Resize the token embeddings because we've just added 3 new tokens \n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def get_stress(phone):\n",
    "    stress = []\n",
    "    for s in phone.split():\n",
    "        if s[-1].isdigit():\n",
    "            if s[-1] == '2':\n",
    "                stress.append(0)\n",
    "            else:\n",
    "                stress.append(int(s[-1]))\n",
    "    return stress\n",
    "\n",
    "def alternating(stress):\n",
    "    #Check if the stress and unstress are alternating\n",
    "    check1 = len(set(stress[::2])) <= 1 and (len(set(stress[1::2])) <= 1)\n",
    "    check2 = len(set(stress)) == 2 if len(stress) >=2 else True\n",
    "    return (check1 and check2)\n",
    "\n",
    "def get_phones(rhyme_word):\n",
    "    phone = pronouncing.phones_for_word(rhyme_word)[0]\n",
    "    stress = get_stress(phone)\n",
    "    p_state = stress[0]\n",
    "    n_syllables = len(stress)\n",
    "    return p_state, n_syllables\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits: Tensor,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    filter_value: float = -float(\"Inf\"),\n",
    "    min_tokens_to_keep: int = 1,\n",
    "    return_index = False\n",
    ") -> Tensor:\n",
    "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    Args:\n",
    "        logits: logits distribution shape (batch size, vocabulary size)\n",
    "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = logits >= torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = indices_keep[0].tolist()\n",
    "        indices_keep = [i for i,x in enumerate(indices_keep) if x == True]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    if return_index == True:\n",
    "        return logits, indices_keep\n",
    "    return logits\n",
    "\n",
    "\n",
    "def reverse_order(line):\n",
    "    line = line.replace(', ', ' , ')\n",
    "    words = line.split()\n",
    "    return ' '.join(reversed(words)).replace(' , ', ', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6397af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_list = ['that','is','of','the','it','a','as','with','like','go','to','on','in','at','are','and']\n",
    "def check_either_stress(stress, source_word, loose = True):\n",
    "    if loose and source_word in loose_list:\n",
    "        return True\n",
    "    if len(stress) == 1 and len(pronouncing.phones_for_word(source_word))>1:\n",
    "                    phone0 = pronouncing.phones_for_word(source_word)[0]\n",
    "                    phone1 = pronouncing.phones_for_word(source_word)[1]\n",
    "                    stress0 = [int(s[-1]) for s in phone0.split() if s[-1].isdigit()]\n",
    "                    stress1 = [int(s[-1]) for s in phone1.split() if s[-1].isdigit()]\n",
    "                    if stress0+stress1 ==1 and stress0*stress1 == 0:\n",
    "                        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e301bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(input_ids1, temperature = 0.85, topk = 100, n_sample=10, device = 'cuda:0'):\n",
    "    current_word = 0\n",
    "    original = tokenizer.decode(input_ids1[0])\n",
    "    for _ in range(1):\n",
    "        outputs1 = model(input_ids1)\n",
    "        #print(outputs1)\n",
    "        next_token_logits1 = outputs1[0][:, -1, :]\n",
    "        next_token_logits1 = top_k_top_p_filtering(next_token_logits1, top_k=topk)\n",
    "        logit_zeros = torch.zeros(len(next_token_logits1)).cuda()\n",
    "        #logit_zeros = torch.zeros(len(next_token_logits1), device=device)\n",
    "\n",
    "        next_token_logits = next_token_logits1 * (1/ temperature)\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=n_sample).squeeze(1)\n",
    "        #unfinished_sents = torch.ones(1, dtype=torch.long, device=device)\n",
    "        unfinished_sents = torch.ones(1, dtype=torch.long).cuda()\n",
    "        tokens_to_add = next_tokens * unfinished_sents + tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "\n",
    "        temp = []\n",
    "        for i in range(len(input_ids1)):\n",
    "            temp +=[torch.cat([input_ids1[i].reshape(1,-1), token_to_add.reshape(1,-1)], dim=-1) for token_to_add in tokens_to_add[i]]\n",
    "        input_ids1 = torch.stack(temp).view(len(temp),-1)\n",
    "        # decode the generated token ids to natural words\n",
    "        results = []\n",
    "        input_ids1_l = []\n",
    "        for input_id1 in input_ids1:\n",
    "            gen = tokenizer.decode(input_id1).replace(original,'').strip(' ')\n",
    "            if len(gen.split()) >0:\n",
    "                gen = gen.split()[0]\n",
    "                gen = gen.lower()\n",
    "                if gen not in results:\n",
    "                    results.append(gen)\n",
    "        return results\n",
    "        '''\n",
    "        if tokenizer.decode(tokens_to_add[0])[0] == ' ':\n",
    "            if current_word ==1:\n",
    "                return tokenizer.decode(input_ids1[0]).split()[-1], False\n",
    "            current_word += 1\n",
    "        input_ids1 = torch.cat([input_ids1, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76c5c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (32): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (33): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (34): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (35): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "gpt2_tokenizer  = AutoTokenizer.from_pretrained('gpt2-large')\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d0036",
   "metadata": {},
   "source": [
    "Update 1201: \n",
    "- dealing with a phrase\n",
    "- improvements of keywords enforcement\n",
    "- sample from the final beam\n",
    "\n",
    "made changes to the below functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc9a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularBeamSearch(prompts):\n",
    "\t'''\n",
    "\tBeam search that considers the coherence by adding a new variable: previously_generated_lines\n",
    "\t'''\n",
    "\tBeamScorer = {}\n",
    "\tfor sentence in prompts:\n",
    "\t\tloss = score_gpt2(sentence)\n",
    "\t\tBeamScorer[sentence] = [loss]\n",
    "\tanswers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "\tnew_prompts = [ans[0] for ans in answers]\n",
    "\treturn new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a8f20155",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "def sample_prompts(prompts,previous='', temperature = 1):\n",
    "    BeamScorer = {}\n",
    "    for sentence in prompts:\n",
    "        loss = score_gpt2(previous+sentence)\n",
    "        BeamScorer[sentence] = [loss]\n",
    "    p = BeamScorer.values()\n",
    "    p = torch.tensor(list(p))*(1/temperature)\n",
    "    try:\n",
    "        p = p.squeeze(1)\n",
    "    except:\n",
    "        pass\n",
    "    p_softmax = torch.nn.functional.softmax(p)\n",
    "    index = torch.multinomial(p_softmax,num_samples=len(prompts))\n",
    "    new_prompts = [prompts[i] for i in index]\n",
    "    return new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b8f6bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_gpt2(sentence, normalize = True):\n",
    "\t'''\n",
    "\tThe default setting is to normalize because we won't face the issue mentioned in function \"score\".\n",
    "\t'''\n",
    "\ttokens_tensor = gpt2_tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")[0].cuda()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss = gpt2_model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "\tif normalize:\n",
    "\t\treturn loss/len(tokens_tensor)\n",
    "\telse:\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "019a1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myBeamSearch(prompts, all_states, all_n_sys, all_keywords, beam_size = 5,enforce_keywords=True):\n",
    "    BeamScorer = {}\n",
    "    return_seq, return_stt, return_sys, return_key = [], [], [], []\n",
    "    \n",
    "    if (not enforce_keywords) or len(all_keywords)==0:\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            loss = score(sentence)\n",
    "            BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "    else:\n",
    "        min_remaining = min([len(x) for x in all_keywords])\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            #start with fewer keywords remaining\n",
    "            if len(keywords) == min_remaining:\n",
    "                loss = score(sentence)\n",
    "                BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "        BeamScorer={}\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            #then\n",
    "            if len(keywords) == min_remaining+1:\n",
    "                loss = score(sentence)\n",
    "                BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers += sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "        BeamScorer={}\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            #last, most keywords remaining\n",
    "            if len(keywords) == min_remaining+2:\n",
    "                loss = score(sentence)\n",
    "                BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers += sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "    new_prompts = [ans[0] for ans in answers]\n",
    "    new_p_states = [ans[1][1] for ans in answers]\n",
    "    new_n_sys = [ans[1][2] for ans in answers]\n",
    "    new_keywords = [ans[1][3] for ans in answers]\n",
    "    l = len(new_prompts)\n",
    "    if l > beam_size:\n",
    "        return_seq += new_prompts[0:beam_size]\n",
    "        return_stt += new_p_states[0:beam_size]\n",
    "        return_sys += new_n_sys[0:beam_size]\n",
    "        return_key += new_keywords[0:beam_size]\n",
    "    else:\n",
    "        return_seq +=new_prompts\n",
    "        return_stt += new_p_states\n",
    "        return_sys += new_n_sys\n",
    "        return_key += new_keywords\n",
    "    return return_seq,return_stt, return_sys, return_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1144e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "562ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sentence, normalize = True):\n",
    "\t'''\n",
    "\tScore a single sentence using the plan-to-lyrics model.\n",
    "\tThe recommended setting is to NOT normalize, because the input sentence is very long: it contains the title, planed keywords, and previously generated lines. \n",
    "\tIn addition, the candidate sentences contain the same prefix (i.e., the title, planed keywords, and previously generated lines) and only differ in the currently generated line.\n",
    "\tNormaling means dividing the loss by a large factor which may result in similarity accross different candidate sentences.\n",
    "\t'''\n",
    "\ttokens_tensor = tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")[0].cuda()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss = model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "\tif normalize:\n",
    "\t\treturn loss/len(tokens_tensor)\n",
    "\telse:\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c32c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stress_phrase(phrase):\n",
    "    words = phrase.split()\n",
    "    stress=[]\n",
    "    for source_word in words:\n",
    "        phone = pronouncing.phones_for_word(source_word)[0]\n",
    "        stress+= get_stress(phone)\n",
    "    return stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "090524aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_character_word = ['i','a']\n",
    "forbidden_words = ['dona','er','ira','ia',\"'s\",\"'m\",\"hmm\",\"mm\"]\n",
    "def get_valid_samples(prompt, p_state, n_syllables, keywords, n_sample=30, n_cands=5):\n",
    "    #if n_syllables == 10 or n_syllables==11:\n",
    "    if n_syllables == 10 and len(keywords)==0:\n",
    "        return [prompt], [p_state], [n_syllables], [keywords]\n",
    "    elif n_syllables > 10:\n",
    "        return [], [], [],[]\n",
    "    states = []\n",
    "    all_n_syl = []\n",
    "    \n",
    "    prompts = []\n",
    "    all_keywords= [] \n",
    "    #insert the keyword whenever possible\n",
    "    for source_word in keywords:\n",
    "        stress = get_stress_phrase(source_word)\n",
    "        #if not alternating(stress):\n",
    "            #continue\n",
    "\n",
    "        #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "        flag = check_either_stress(stress, source_word)\n",
    "\n",
    "        if (stress[-1] == 1- p_state or flag) and (n_syllables+len(stress)<=10):\n",
    "            states.append(stress[0])\n",
    "            all_n_syl.append(n_syllables+len(stress))\n",
    "            #print(source_word)\n",
    "            prompts.append(prompt+ ' ' +reverse_order(source_word))\n",
    "            copy = keywords.copy()\n",
    "            copy.remove(source_word)\n",
    "            all_keywords.append(copy)    \n",
    "    \n",
    "    #The normal process of decoding\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    tokens = generate_next_word(input_ids, n_sample=n_sample)\n",
    "    #print(tokens)\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if (len(token) == 1 and token not in single_character_word) or token in forbidden_words:\n",
    "            continue\n",
    "        if token not in prompt:\n",
    "            try:\n",
    "                phone = pronouncing.phones_for_word(token)[0]\n",
    "                stress = get_stress(phone)\n",
    "            except:\n",
    "                continue\n",
    "            if (not alternating(stress)) or (len(stress)==0):\n",
    "                continue\n",
    "\n",
    "            #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "            flag = check_either_stress(stress, token)\n",
    "            if n_syllables+len(stress)<=10:\n",
    "                if (stress[-1] == 1- p_state) or flag:\n",
    "                    tokens.append(token)\n",
    "                    if stress[-1] == 1- p_state:\n",
    "                        states.append(stress[0])\n",
    "                    elif flag:\n",
    "                        states.append(1- p_state)\n",
    "                    all_n_syl.append(n_syllables+len(stress))\n",
    "                    prompts.append(prompt+ ' ' + token )\n",
    "                    all_keywords.append(keywords)\n",
    "                    if len(prompts)>= n_cands:\n",
    "                        return prompts, states, all_n_syl, all_keywords\n",
    "    return prompts, states, all_n_syl, all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "45e0b258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "9200640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'falling', 'future']\n",
      "['look at you with falling snow and future', 'look at me with falling snow and future', 'know that there is falling snow and future', 'up at me with falling snow and future', 'looks at you with falling snow and future', 'this is just the falling snow and future', 'that is just the falling snow and future', 'looks at me with falling snow and future', 'say that there is falling snow and future', 'said that there is falling snow and future', 'do with just the falling snow and future', 'down at me with falling snow and future', 'think about the falling snow and future', 'think that there is falling snow and future', 'you with just the falling snow and future', 'up at you with falling snow and future', 'see that there is falling snow and future', 'here is just the falling snow and future', 'down at you with falling snow and future', 'comes with just the falling snow and future']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [00:10<02:10, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['winter', 'is', 'coming']\n",
      "['like they are said that winter is coming', 'reports are said that winter is coming', 'are telling you that winter is coming', 'are experts say that winter is coming', 'always know that winter is whats coming', 'never know that winter is whats coming', 'that you are thinking winter is coming', 'are always said that winter is coming', 'even know that winter is whats coming', 'already know that winter is coming', 'like you are thinking winter is coming', 'are prophets say that winter is coming', 'really know that winter is whats coming', 'like prophets say that winter is coming']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [00:20<02:03, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gather', 'honest like a sir', 'humor']\n",
      "['gather', 'honest like a sir', 'humor']\n",
      "['gather', 'honest like a sir', 'humor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [00:40<02:39, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['honest like a sir go gather humor']\n",
      "['spring', 'happy', 'blooming']\n",
      "['here with happy spring and even blooming', 'filled with happy spring and even blooming', 'all with happy spring and even blooming', 'bloss with happy spring and even blooming', 'peak with happy spring and even blooming', 'season comes with spring and happy blooming', 'always comes with spring and happy blooming', 'also comes with spring and happy blooming', 'filled with spring alive and happy blooming', 'really comes with spring and happy blooming', 'came with spring alive and happy blooming', 'years with spring alive and happy blooming', 'always packed with spring and happy blooming', 'season packed with spring and happy blooming', 'day with spring alive and happy blooming', 'do with spring alive and happy blooming', 'flowers here with spring and happy blooming', 'weather comes with spring and happy blooming', 'getting packed with spring and happy blooming', 'being here with spring and happy blooming']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:50<02:09, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air', 'heat', 'warm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [01:02<01:52, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are almost any air that heat is warm', 'are given heat that climate air is warm', 'are having heat that climate air is warm']\n",
      "['little', 'birds', 'may']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [01:13<01:37, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go under there like little birds the may', 'go over there like little birds the may', 'like little ary season birds the may', 'like little hunting season birds the may', 'however little birds enjoy the may', 'however little just like birds the may']\n",
      "['flowers', 'leaves', 'storm']\n",
      "['go out with yellow flowers leaves and storm', 'go green with yellow flowers leaves and storm', 'with fields like flowers autumn leaves and storm', 'with yellow flowers autumn leaves and storm', 'with autumn leaves and yellow flowers storm', 'like autumn leaves and yellow flowers storm', 'like autumn leaves with yellow flowers storm', 'with vibrant yellow flowers leaves and storm', 'like yellow flowers autumn leaves and storm', 'like watching flowers autumn leaves and storm', 'with yellow flowers fallen leaves and storm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [01:26<01:26, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summer', 'moon', 'day']\n",
      "['like what is over moon that summer day', 'is pretty much like summer moon that day', 'like she is over moon that summer day', 'is something much like summer moon that day', 'are going through that summer moon like day', 'is going through that summer moon like day', 'are pretty much like summer moon that day', 'are four is over moon that summer day', 'like what is over summer moon that day', 'like any other moon that summer day', 'is shining over moon that summer day', 'is always over moon that summer day', 'are always over moon that summer day', 'is any other moon that summer day', 'is over moon that lovely summer day']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [01:39<01:14, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue', 'sky', 'clouds']\n",
      "['blue', 'sky', 'clouds']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [02:03<01:20, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about whats blue the sky are fluffy clouds', 'whats little blue the sky are quiet clouds']\n",
      "['sudden like a flash', 'rain', 'thunder']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [02:12<00:56, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sudden like a flash with rain and thunder', 'sudden like a flash and rain with thunder', 'rain and sudden like a flash with thunder']\n",
      "['Summer', 'fill', 'crowds']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [02:24<00:39, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go out that never fill is Summer crowds', 'is going fill that endless Summer crowds', 'that almost never fill is Summer crowds', 'is always fill that Summer endless crowds']\n",
      "['Spring', 'no', 'wonder']\n",
      "['goers Spring are longer no the wonder', 'after Spring are longer no the wonder', 'into Spring are really no the wonder', 'rolling Spring are longer no the wonder', 'really no are bringing Spring the wonder', 'flowers Spring are really no the wonder', 'seasons no are bringing Spring the wonder', 'early Spring are really no the wonder', 'cleaning Spring are longer no the wonder', 'no already bringing Spring the wonder', 'into Spring discover no the wonder', 'season Spring discover no the wonder', 'cleaning Spring discover no the wonder', 'welcomed Spring discover no the wonder']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [02:35<00:25, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seasons', 'years', 'keep']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [02:46<00:12, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with other things like years and seasons keep', 'with many things like years and seasons keep', 'with story just like years and seasons keep', 'surprises just like years and seasons keep', 'important things like years and seasons keep']\n",
      "['future', 'months', 'reap']\n",
      "['go out with months that harvest future reap', 'that they go months with seeing future reap', 'go up with months that harvest future reap', 'go up with months that seeing future reap', 'that many months with seeing future reap', 'that seven months with future harvest reap', 'go months ahead with seeing future reap', 'with only months that harvest future reap', 'at spending months with seeing future reap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:57<00:00, 12.66s/it]\n"
     ]
    }
   ],
   "source": [
    "four_seasons_story_line = [\n",
    "['snow', 'falling', 'future'],\n",
    "['winter', 'is', 'coming'],\n",
    "['gather', 'honest like a sir', 'humor'],\n",
    "['spring', 'happy', 'blooming'],\n",
    "['air', 'heat', 'warm'],\n",
    "['little', 'birds', 'may'],\n",
    "['flowers', 'leaves', 'storm'],\n",
    "['summer','moon', 'day'],\n",
    "['blue', 'sky', 'clouds'],\n",
    "['sudden like a flash', 'rain', 'thunder'],\n",
    "['Summer', 'fill', 'crowds'],\n",
    "['Spring', 'no', 'wonder'],\n",
    "['seasons','years', 'keep'],\n",
    "['future', 'months', 'reap']]\n",
    "\n",
    "\n",
    "example_title = 'Four Seasons'\n",
    "beam_size=20\n",
    "previous = \"\"\n",
    "enforce_keywords = True\n",
    "for kws in tqdm(four_seasons_story_line):\n",
    "    success=False\n",
    "    n_sample = 30\n",
    "    while success != True:\n",
    "        print(kws)\n",
    "        rhyme_word = kws[-1]\n",
    "        prefix =  '''Keywords: ''' + '; '.join(kws) +'. Sentence in reverse order: '\n",
    "        prompt = '''<|startoftext|> Title: ''' + example_title + ' ' + ','.join(previous.split(',')[-3:]) + prefix + rhyme_word\n",
    "        #prompt = '''<|startoftext|> Title: ''' + example_title + ' ' + prefix + rhyme_word\n",
    "        p_state, n_syllables = get_phones(rhyme_word)\n",
    "        result_list = []\n",
    "        i=0\n",
    "        prompts, all_states, all_n_sys, all_keywords = get_valid_samples(prompt,p_state, n_syllables, keywords = kws[:2], n_sample=n_sample,n_cands=5)\n",
    "        while i<7:\n",
    "            #print(i)\n",
    "            new_prompts, new_states, new_n_sys, new_keywords = [], [], [], []\n",
    "            for prompt, p_state, n_syllables, keyword in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "                t_p, t_state, t_sys, t_keywords = get_valid_samples(prompt, p_state, n_syllables, keyword,n_sample=n_sample)\n",
    "                new_prompts+=t_p\n",
    "                new_states+=t_state\n",
    "                new_n_sys+=t_sys\n",
    "                new_keywords+=t_keywords\n",
    "            prompts, all_states, all_n_sys, all_keywords = new_prompts, new_states, new_n_sys, new_keywords\n",
    "            prompts, all_states, all_n_sys, all_keywords = myBeamSearch(prompts,all_states, all_n_sys, all_keywords, beam_size=beam_size, enforce_keywords=enforce_keywords)\n",
    "            i += 1\n",
    "        if len(prompts)==0:\n",
    "            if n_sample>300:\n",
    "                print('Failed to generate valid samples. Please try re-generation for this line.')\n",
    "                previous += '   ,'\n",
    "                break\n",
    "            n_sample = n_sample*3\n",
    "\n",
    "        else:\n",
    "            correct_prompts = [reverse_order(p.split('order: ')[1]) for p in prompts]\n",
    "            print(correct_prompts)\n",
    "            result_list = sample_prompts(correct_prompts, previous)\n",
    "            \n",
    "            success=True\n",
    "            found = False \n",
    "            for r in result_list:\n",
    "                if kws[0] in r or kws[1] in r:\n",
    "                    previous = previous + r + ','\n",
    "                    found = True\n",
    "                    break\n",
    "            if found == False:\n",
    "                    previous = previous + result_list[0]+','\n",
    "                    n_sample = n_sample*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "22ef9703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce keywords:\n",
      "\n",
      "comes with just the falling snow and future,\n",
      "that you are thinking winter is coming,\n",
      "honest like a sir go gather humor,\n",
      "peak with happy spring and even blooming,\n",
      "are given heat that climate air is warm,\n",
      "however little birds enjoy the may,\n",
      "go out with yellow flowers leaves and storm,\n",
      "is going through that summer moon like day,\n",
      "whats little blue the sky are quiet clouds,\n",
      "rain and sudden like a flash with thunder,\n",
      "is going fill that endless Summer crowds,\n",
      "into Spring are really no the wonder,\n",
      "surprises just like years and seasons keep,\n",
      "go months ahead with seeing future reap,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Enforce keywords:\\n')\n",
    "\n",
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e9344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
