{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f883d3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPTNeoForCausalLM, GPTNeoModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pronouncing\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894db4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb2c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c8fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed to a fixed value to get reproducible results \n",
    "torch.manual_seed(42)\n",
    "# Download the pre-trained GPT-Neo model's tokenizer\n",
    "# Add the custom tokens denoting the beginning and the end \n",
    "# of the sequence and a special token for padding\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", \n",
    "                                          bos_token=\"<|startoftext|>\",\n",
    "                            eos_token=\"<|endoftext|>\",\n",
    "                            pad_token=\"<|pad|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c27d817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 2048)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pre-trained GPT-Neo model and transfer it to the GPU\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"news-gpt-neo-1.3B-keywords-line-by-line-reverse/checkpoint-15000\").cuda()\n",
    "# Resize the token embeddings because we've just added 3 new tokens \n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07151c5a",
   "metadata": {},
   "source": [
    "### rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab17501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19cdd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stress(phone):\n",
    "    stress = []\n",
    "    for s in phone.split():\n",
    "        if s[-1].isdigit():\n",
    "            if s[-1] == '2':\n",
    "                stress.append(0)\n",
    "            else:\n",
    "                stress.append(int(s[-1]))\n",
    "    return stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af38ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating(stress):\n",
    "    #Check if the stress and unstress are alternating\n",
    "    check1 = len(set(stress[::2])) <= 1 and (len(set(stress[1::2])) <= 1)\n",
    "    check2 = len(set(stress)) == 2 if len(stress) >=2 else True\n",
    "    return (check1 and check2)\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c912af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phones(rhyme_word):\n",
    "    phone = pronouncing.phones_for_word(rhyme_word)[0]\n",
    "    stress = get_stress(phone)\n",
    "    p_state = stress[0]\n",
    "    n_syllables = len(stress)\n",
    "    return p_state, n_syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b21ea3",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0823dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits: Tensor,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    filter_value: float = -float(\"Inf\"),\n",
    "    min_tokens_to_keep: int = 1,\n",
    "    return_index = False\n",
    ") -> Tensor:\n",
    "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    Args:\n",
    "        logits: logits distribution shape (batch size, vocabulary size)\n",
    "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = logits >= torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = indices_keep[0].tolist()\n",
    "        indices_keep = [i for i,x in enumerate(indices_keep) if x == True]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    if return_index == True:\n",
    "        return logits, indices_keep\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39298237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a complete word instead of subwords\n",
    "def generate_next_word(input_ids1, temperature = 0.85, topk = 100, device = 'cuda:0'):\n",
    "        current_word = 0\n",
    "        for _ in range(10):\n",
    "            outputs1 = model(input_ids1)\n",
    "            next_token_logits1 = outputs1[0][:, -1, :]\n",
    "            next_token_logits1 = top_k_top_p_filtering(next_token_logits1, top_k=topk)\n",
    "            logit_zeros = torch.zeros(len(next_token_logits1)).cuda()\n",
    "            #logit_zeros = torch.zeros(len(next_token_logits1), device=device)\n",
    "\n",
    "            next_token_logits = next_token_logits1 * temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            #unfinished_sents = torch.ones(1, dtype=torch.long, device=device)\n",
    "            unfinished_sents = torch.ones(1, dtype=torch.long).cuda()\n",
    "            tokens_to_add = next_tokens * unfinished_sents + tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "\n",
    "            if tokenizer.eos_token_id in next_tokens[0]:\n",
    "                input_ids1 = torch.cat([input_ids1, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "                return '', True\n",
    "\n",
    "            if tokenizer.decode(tokens_to_add[0])[0] == ' ':\n",
    "                if current_word ==1:\n",
    "                    return tokenizer.decode(input_ids1[0]).split()[-1], False\n",
    "                current_word += 1\n",
    "            input_ids1 = torch.cat([input_ids1, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a726d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_samples(prompt, p_state, n_syllables, keywords):\n",
    "    states = []\n",
    "    all_n_syl = []\n",
    "    \n",
    "    prompts = []\n",
    "    all_keywords = [] \n",
    "    #insert the keyword whenever possible\n",
    "    for source_word in keywords:\n",
    "        phone = pronouncing.phones_for_word(source_word)[0]\n",
    "        stress = get_stress(phone)\n",
    "        if not alternating(stress):\n",
    "            continue\n",
    "\n",
    "        #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "        flag = check_either_stress(stress, source_word)\n",
    "\n",
    "        if stress[-1] == 1- p_state or flag:\n",
    "            states.append(stress[0])\n",
    "            all_n_syl.append(n_syllables+len(stress))\n",
    "            prompts.append(prompt+ ' ' + source_word )\n",
    "            copy = keywords.copy()\n",
    "            copy.remove(source_word)\n",
    "            all_keywords.append(copy)    \n",
    "    \n",
    "    #The normal process of decoding\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    tokens = []\n",
    "    while len(tokens) < 3:\n",
    "        token, eos = generate_next_word(input_ids)\n",
    "        if (token not in tokens) and (token not in keywords):\n",
    "            #print(token, tokens)\n",
    "            try:\n",
    "                phone = pronouncing.phones_for_word(token)[0]\n",
    "                stress = get_stress(phone)\n",
    "                if not alternating(stress):\n",
    "                    continue\n",
    "\n",
    "                #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "                flag = check_either_stress(stress, token)\n",
    "\n",
    "                if stress[-1] == 1- p_state or flag:\n",
    "                    tokens.append(token)\n",
    "                    states.append(stress[0])\n",
    "                    all_n_syl.append(n_syllables+len(stress))\n",
    "                    prompts.append(prompt+ ' ' + token )\n",
    "                    all_keywords.append(keywords)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return prompts, states, all_n_syl, all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd68de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_either_stress(stress, source_word, loose = False):\n",
    "    if loose:\n",
    "        return len(stress) == 1\n",
    "    if len(stress) == 1 and len(pronouncing.phones_for_word(source_word))>1:\n",
    "                    phone0 = pronouncing.phones_for_word(source_word)[0]\n",
    "                    phone1 = pronouncing.phones_for_word(source_word)[1]\n",
    "                    stress0 = [int(s[-1]) for s in phone0.split() if s[-1].isdigit()]\n",
    "                    stress1 = [int(s[-1]) for s in phone1.split() if s[-1].isdigit()]\n",
    "                    if stress0+stress1 ==1 and stress0*stress1 == 0:\n",
    "                        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3164093f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be8dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_order(line):\n",
    "    line = line.replace(', ', ' , ')\n",
    "    words = line.split()\n",
    "    return ' '.join(reversed(words)).replace(' , ', ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a53961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "score_model = model\n",
    "def myBeamSearch(true_beams, beam_size = 5):\n",
    "    BeamScorer = {}\n",
    "    for sentence in true_beams:\n",
    "        tokenize_input = tokenizer.tokenize(sentence)\n",
    "        tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "\n",
    "        tensor_input = tensor_input.to(device)\n",
    "        loss = score_model(tensor_input, labels=tensor_input)\n",
    "        avg_lp = torch.tensor(-loss[0].item()/len(tokenize_input))\n",
    "        BeamScorer[sentence] = avg_lp\n",
    "    BeamScorer = {k: v for k, v in sorted(BeamScorer.items(), key=lambda x: x[1], reverse=True)}\n",
    "    return list(BeamScorer.keys())[:beam_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb3b882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 5\n",
    "def gen_recursion(prompt, p_state, n_syllables, keywords):\n",
    "    global result_list\n",
    "    '''I modified this criterion to speed up the example.\n",
    "    I suggest to add non-repeat-unigram (= 3) and keyword checking\n",
    "    '''\n",
    "    if n_syllables >= 5: \n",
    "        line = prompt.split(': ')[-1]\n",
    "        reversed_words = reverse_order(line)\n",
    "        reversed_words = reversed_words\n",
    "        result_list.append(reversed_words)\n",
    "        if len(result_list)>=beam_size:\n",
    "            result_list = myBeamSearch(result_list, beam_size = beam_size)\n",
    "            #print(result_list)\n",
    "        return result_list\n",
    "    prompts, states, all_n_sys, all_keywords = get_valid_samples(prompt,p_state, n_syllables, keywords)\n",
    "    for prompt,p_state, n_syllables, keyword in zip(prompts, states, all_n_sys, all_keywords):\n",
    "        gen_recursion(prompt,p_state, n_syllables, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf61f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "four_seasons_story_line = [\n",
    "['snow', 'falling', 'future'],\n",
    "['winter', 'is', 'coming'],\n",
    "['gather', 'honest', 'humor'],\n",
    "['spring', 'happy', 'blooming'],\n",
    "['air', 'heat', 'warm'],\n",
    "['little', 'birds', 'may'],\n",
    "['flowers', 'leaves', 'storm'],\n",
    "['summer','moon', 'day'],\n",
    "['blue', 'sky', 'clouds'],\n",
    "['sudden', 'rain', 'thunder'],\n",
    "['Summer', 'fill', 'crowds'],\n",
    "['Spring', 'no', 'wonder'],\n",
    "['seasons','years', 'keep'],\n",
    "['future', 'months', 'reap']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f41dce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air', 'heat', 'warm']\n",
      "['thick and moist and warm', 'close and safe and warm', 'light and safe and warm', 'air and safe and warm', 'heat and safe and warm']\n",
      "['flowers', 'leaves', 'storm']\n",
      "['breathe and air the storm', \"wouldn't be a storm\", 'stir and heat a storm', 'hills and air the storm', 'heat and air the storm']\n",
      "['blue', 'sky', 'clouds']\n",
      "['fills the air the clouds', 'air and air the clouds', 'stars and air the clouds', 'moon and air the clouds', 'heat and air the clouds']\n",
      "['seasons', 'years', 'keep']\n",
      "['stuck in it and keep', 'heat the heat and keep', 'of the air and keep', 'air the heat and keep', 'face the heat and keep']\n"
     ]
    }
   ],
   "source": [
    "example_title = 'The Four Seasons'\n",
    "\n",
    "previous = ''\n",
    "for kws in four_seasons_story_line[:4]:\n",
    "    print(kws)\n",
    "    rhyme_word = kws[-1]\n",
    "    prefix =  '''Keywords: ''' + '; '.join(kws) +'. Sentence in reverse order: '\n",
    "    prompt = '''<|startoftext|> Title: ''' + example_title + ' ' previous + prefix + rhyme_word\n",
    "    p_state, n_syllables = get_phones(rhyme_word)\n",
    "    result_list = []\n",
    "    #to add hard constraints, specify keywords, otherwise use keywords = []\n",
    "    gen_recursion(prompt, p_state, n_syllables, keywords = ['air','heat'])\n",
    "    print(result_list)\n",
    "    \n",
    "    previous = previous + result_list[0] + ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0be5ac07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thick and moist and warm, breathe and air the storm, fills the air the clouds, stuck in it and keep, '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
