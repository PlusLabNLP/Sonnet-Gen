{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fee481eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_seasons_story_line = [\n",
    "['snow', 'falling', 'future'],\n",
    "['winter', 'is', 'coming'],\n",
    "['gather', 'honest', 'humor'],\n",
    "['spring', 'happy', 'blooming'],\n",
    "['air', 'heat', 'warm'],\n",
    "['little', 'birds', 'may'],\n",
    "['flowers', 'leaves', 'storm'],\n",
    "['summer','moon', 'day'],\n",
    "['blue', 'sky', 'clouds'],\n",
    "['sudden', 'rain', 'thunder'],\n",
    "['Summer', 'fill', 'crowds'],\n",
    "['Spring', 'no', 'wonder'],\n",
    "['seasons','years', 'keep'],\n",
    "['future', 'months', 'reap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5229cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import src.data.data as data\n",
    "import src.data.config as cfg\n",
    "import src.interactive.functions as interactive\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "cfg.device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712e1f5",
   "metadata": {},
   "source": [
    "### Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3da6550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/conceptnet/processed/generation/rel_language-trainsize_100-devversion_12-maxe1_10-maxe2_15-maxr_5.pickle\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model_file =  'pretrained_models/reverse_comet_1e-05_adam_32_20000.pickle'\n",
    "opt, state_dict = interactive.load_model_file(model_file)\n",
    "data_loader, text_encoder = interactive.load_data(\"conceptnet\", opt)\n",
    "\n",
    "n_ctx = data_loader.max_e1 + data_loader.max_e2 + data_loader.max_r\n",
    "n_vocab = len(text_encoder.encoder) + n_ctx\n",
    "\n",
    "model = interactive.make_model(opt,  40543, 29, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985f59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getloss(input_e1, input_e2, relation, prnt = False):\n",
    "    if relation not in data.conceptnet_data.conceptnet_relations:\n",
    "        if relation == \"common\":\n",
    "            relation = common_rels\n",
    "        else:\n",
    "            relation = \"all\"\n",
    "    outputs = interactive.evaluate_conceptnet_sequence(\n",
    "        input_e1, model, data_loader, text_encoder, relation, input_e2)\n",
    "\n",
    "    for key, value in outputs.items():\n",
    "        if prnt:\n",
    "            print(\"{} \\t {} {} {} \\t\\t norm: {:.4f} \\t\".format(\n",
    "                input_e1, key, rel_formatting[key], input_e2, value['normalized_loss']))\n",
    "        return round(value['normalized_loss'],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2645e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPred(input_event, relation, prnt = True, sampling_algorithm = 'beam-2'):\n",
    "    sampler = interactive.set_sampler(opt, sampling_algorithm, data_loader)\n",
    "    outputs = interactive.get_conceptnet_sequence(input_event, model, sampler, data_loader, text_encoder, relation, prnt)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94e12033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing {'snow': 'white', 'sky': 'sun', 'flowers': 'rose', 'Summer': 'autumn', 'summer': 'summer solstice in washington'}\n"
     ]
    }
   ],
   "source": [
    "#randomly sample at most N=5 nouns, not from the same line\n",
    "#then, select the most confident M candidates to do the replacement\n",
    "N = 5\n",
    "M = 2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    location_dict = {}\n",
    "    for i, keywords in enumerate(four_seasons_story_line):\n",
    "        w1, w2, _ = keywords\n",
    "        ent = nlp(w1)[0]\n",
    "        if ent.pos_ == 'NOUN':\n",
    "            location_dict[str(ent)] = [i,0]\n",
    "            continue\n",
    "        ent = nlp(w2)[0]\n",
    "        if ent.pos_ == 'NOUN':\n",
    "            location_dict[str(ent)] = [i,1]\n",
    "    samples = random.sample(location_dict.keys(),N)\n",
    "    relations = ['SymbolOf']\n",
    "    score_dict = {}\n",
    "    replace_dict = {}\n",
    "    polished_lines = []\n",
    "    flatten_list = [j for sub in four_seasons_story_line for j in sub]\n",
    "    for ent in samples:\n",
    "        result = getPred(ent, relation=relations, sampling_algorithm = 'topk-10', prnt = False)[relations[0]]['beams']\n",
    "        for i in range(len(result)):\n",
    "            if result[i] not in flatten_list:\n",
    "                result = result[i]\n",
    "                break\n",
    "        score_dict[ent] = getloss(ent, result, 'SymbolOf', prnt = False)\n",
    "        replace_dict[ent] = result\n",
    "\n",
    "    selected = sorted(score_dict.items(), key=lambda item: item[1])[:M]\n",
    "    print(f\"replacing {replace_dict}\")\n",
    "    for ent in selected:\n",
    "        ent = ent[0]\n",
    "        location = location_dict[ent]\n",
    "        polished_lines.append(location[0])\n",
    "        four_seasons_story_line[location[0]][location[1]] = replace_dict[ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49a6f9",
   "metadata": {},
   "source": [
    "Ashima TODOðŸ‘†: instead of randomly select, include the user in the word selection process. again, list same for simile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e3119",
   "metadata": {},
   "source": [
    "### Simile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d77b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.bart import BARTModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "datadir = 'simile'\n",
    "cpdir = datadir+'/'\n",
    "bart = BARTModel.from_pretrained(cpdir,checkpoint_file='checkpoint-simile/checkpoint_best.pt',data_name_or_path=datadir)\n",
    "\n",
    "bart.cuda()\n",
    "bart.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1c98902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "18802945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stress(phone):\n",
    "    stress = []\n",
    "    for s in phone.split():\n",
    "        if s[-1].isdigit():\n",
    "                stress.append(int(s[-1]))\n",
    "    for i, number in enumerate(stress):\n",
    "        if number==2:\n",
    "            if i==0:\n",
    "                stress[0] = 1-stress[1]\n",
    "            else:\n",
    "                stress[i] = 1-stress[i-1]\n",
    "    return stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14857883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating(stress):\n",
    "    #Check if the stress and unstress are alternating\n",
    "    check1 = len(set(stress[::2])) <= 1 and (len(set(stress[1::2])) <= 1)\n",
    "    check2 = len(set(stress)) == 2 if len(stress) >=2 else True\n",
    "    return (check1 and check2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "92ebd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_none_phrase(vehicle):\n",
    "    if vehicle.startswith('a '):\n",
    "        vehicle = vehicle.replace('a ','')\n",
    "    doc = nlp(vehicle)\n",
    "    flag = False\n",
    "    for ent in doc:\n",
    "        if ent.pos_== 'NOUN' or ent.pos_=='PROPN':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e7e83",
   "metadata": {},
   "source": [
    "Ashima TODOðŸ‘†: please check if you can install allennlp, if yes, we should use https://demo.allennlp.org/constituency-parsing to detect the noun phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4f1ee2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_meter(adj, vehicle):\n",
    "    phone = pronouncing.phones_for_word(adj)[0]\n",
    "    stress_adj = get_stress(phone)\n",
    "    vehicle = vehicle.strip(',.<>')\n",
    "    try:\n",
    "        if len(vehicle.split())==1:\n",
    "            phone = pronouncing.phones_for_word(vehicle)[0]\n",
    "            stress_vehicle = get_stress(phone)\n",
    "        else:\n",
    "            stress_vehicle = []\n",
    "            for word in vehicle.split():\n",
    "                phone = pronouncing.phones_for_word(word)[0]\n",
    "                stress_vehicle += get_stress(phone)\n",
    "        #assume 'like' can be either stressed or unstressed\n",
    "        if stress_vehicle[0]==stress_adj[-1] and alternating(stress_vehicle):\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "91ac2f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_meter('incredible','a miracle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "bd115d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_none_phrase('a miracle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2dbd3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simile_vehicle(inp, t=0.7):\n",
    "        simile_phrases=[]\n",
    "        prefix = inp+' like'\n",
    "        inputs = [prefix]*10\n",
    "        l=len(tok(prefix)[0])\n",
    "        hypotheses_batch = bart.sample(inputs,sampling=True, sampling_topk=5, temperature=t, max_len_b=l+2, min_len=l)\n",
    "        for hypothesis in hypotheses_batch:\n",
    "            vehicle = hypothesis.split(' like ')[1].split('<')[0].lower()\n",
    "            print(vehicle)\n",
    "            if is_none_phrase(vehicle) and check_meter(inp, vehicle):\n",
    "                simile_phrases.append(' '.join([inp, 'like', vehicle]))\n",
    "        return list(set(simile_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f999e29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a superhero\n",
      "a super\n",
      "a god\n",
      "a super\n",
      "a super\n",
      "a superhero\n",
      "a superhero\n",
      "a superhero\n",
      "a miracle\n",
      "a superhero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['incredible like a god', 'incredible like a superhero']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simile_vehicle('incredible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e685cdb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a polite gentleman\n",
      "a gentlemen\n",
      "a proper gentleman\n",
      "a nice lady\n",
      "a polite way to\n",
      "a \"please\"\n",
      "a polite way to\n",
      "a \"nice guy\n",
      "a gentleman\n",
      "a polite handshake\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simile_vehicle('polite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8d3fe",
   "metadata": {},
   "source": [
    "Ashima TODO: involve the users in this decision process ðŸ‘†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5d3e7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a sir\n",
      "a true\n",
      "a sir\n",
      "a true\n",
      "a true\n",
      "a true\n",
      "a sir\n",
      "a real\n",
      "a true\n",
      "a true\n",
      "['honest like a sir']\n",
      "a flash\n",
      "a blink\n",
      "a flash\n",
      "a flash\n",
      "a flash\n",
      "a flash\n",
      "a flash\n",
      "a flash\n",
      "a blink\n",
      "a flash\n",
      "['sudden like a flash']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    location_dict = {}\n",
    "    for i, keywords in enumerate(four_seasons_story_line):\n",
    "        if i not in polished_lines:\n",
    "            w1, w2, _ = keywords\n",
    "            ent = nlp(w1)[0]\n",
    "            if ent.pos_ == 'ADJ':\n",
    "                location_dict[str(ent)] = [i,0]\n",
    "                continue\n",
    "            ent = nlp(w2)[0]\n",
    "            if ent.pos_ == 'ADJ':\n",
    "                location_dict[str(ent)] = [i,1]\n",
    "                \n",
    "    samples = random.sample(location_dict.keys(),M)\n",
    "    for ent in samples:\n",
    "        simile_phrase = simile_vehicle(ent)\n",
    "        print(simile_phrase)\n",
    "        location = location_dict[ent]\n",
    "        polished_lines.append(location[0])\n",
    "        four_seasons_story_line[location[0]][location[1]] = simile_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de8e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
